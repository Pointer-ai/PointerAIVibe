// AI Assistant æœåŠ¡å±‚

import { getAPIConfig } from '../../modules/profileSettings/service'
import { AssistantConfig, ChatSession, LearningProgress, ChatMessage } from './types'
import { log, error } from '../../utils/logger'
import { getProfileData, setProfileData } from '../../utils/profile'
import { addActivityRecord } from '../../modules/profileSettings/service'

/**
 * è·å–ç”¨æˆ·å®Œæ•´çš„å­¦ä¹ ä¸Šä¸‹æ–‡
 */
const getUserCompleteContext = async (): Promise<string> => {
  try {
    // è·å–æ ¸å¿ƒæ•°æ®æ¨¡å—
    const { getLearningGoals, getLearningPaths, getCourseUnits, getAbilityProfile } = await import('../../modules/coreData')
    const { getCurrentProfile } = await import('../../utils/profile')
    const { getCurrentAssessment } = await import('../../modules/abilityAssess/service')
    
    // è·å–å„ç§æ•°æ®
    const goals = getLearningGoals()
    const paths = getLearningPaths()
    const courseUnits = getCourseUnits()
    const abilityProfile = getAbilityProfile()
    const currentProfile = getCurrentProfile()
    const currentAssessment = getCurrentAssessment() // è·å–è¯¦ç»†è¯„ä¼°æ•°æ®
    
    // ä»è¯„ä¼°æ•°æ®ä¸­æå–ä¼˜åŠ¿å’Œå¼±åŠ¿
    let strengthsInfo = 'æœªåˆ†æ'
    let weaknessesInfo = 'æœªåˆ†æ'
    
    if (currentAssessment?.report) {
      strengthsInfo = currentAssessment.report.strengths?.join(', ') || 'æœªåˆ†æ'
      weaknessesInfo = currentAssessment.report.improvements?.join(', ') || 'æœªåˆ†æ'
    }
    
    // ç»„è£…ä¸Šä¸‹æ–‡ä¿¡æ¯
    const context = `
ğŸ§‘â€ğŸ’¼ ç”¨æˆ·æ¡£æ¡ˆä¿¡æ¯:
- æ¡£æ¡ˆåç§°: ${currentProfile?.name || 'æœªè®¾ç½®'}
- æ˜¯å¦å®Œæˆèƒ½åŠ›è¯„ä¼°: ${abilityProfile ? 'æ˜¯' : 'å¦'}
${abilityProfile ? `- æ€»ä½“èƒ½åŠ›è¯„åˆ†: ${abilityProfile.overallScore}/100
- ä¼˜åŠ¿é¢†åŸŸ: ${strengthsInfo}
- å¾…æ”¹è¿›é¢†åŸŸ: ${weaknessesInfo}` : ''}

ğŸ“‹ å­¦ä¹ ç›®æ ‡ç°çŠ¶:
- æ€»ç›®æ ‡æ•°: ${goals.length}ä¸ª
- æ¿€æ´»ç›®æ ‡: ${goals.filter(g => g.status === 'active').length}ä¸ª
- è¿›è¡Œä¸­ç›®æ ‡: ${goals.filter(g => g.status === 'active').map(g => `"${g.title}" (${g.category}, ${g.targetLevel})`).join(', ') || 'æ— '}
- æš‚åœç›®æ ‡: ${goals.filter(g => g.status === 'paused').length}ä¸ª
- å·²å®Œæˆç›®æ ‡: ${goals.filter(g => g.status === 'completed').length}ä¸ª

ğŸ›¤ï¸ å­¦ä¹ è·¯å¾„ç°çŠ¶:
- æ€»è·¯å¾„æ•°: ${paths.length}ä¸ª  
- æ¿€æ´»è·¯å¾„: ${paths.filter(p => p.status === 'active').length}ä¸ª
- å½“å‰å­¦ä¹ è·¯å¾„: ${paths.filter(p => p.status === 'active').map(p => `"${p.title}" (${p.nodes?.length || 0}ä¸ªèŠ‚ç‚¹)`).join(', ') || 'æ— '}

ğŸ“š è¯¾ç¨‹å†…å®¹ç°çŠ¶:
- æ€»è¯¾ç¨‹å•å…ƒ: ${courseUnits.length}ä¸ª
- è¯¾ç¨‹ç±»å‹åˆ†å¸ƒ: ${getContentTypeDistribution(courseUnits)}
- æœ€è¿‘åˆ›å»º: ${courseUnits.length > 0 ? courseUnits[courseUnits.length - 1].title : 'æ— '}

ğŸ’¡ å­¦ä¹ å»ºè®®:
${generateContextualSuggestions(goals, paths, courseUnits, abilityProfile)}
`.trim()
    
    return context
  } catch (error) {
    log('[Context] Failed to get user context:', error)
    return 'âš ï¸ æ— æ³•è·å–ç”¨æˆ·ä¸Šä¸‹æ–‡æ•°æ®'
  }
}

/**
 * è·å–è¯¾ç¨‹å†…å®¹ç±»å‹åˆ†å¸ƒ
 */
const getContentTypeDistribution = (courseUnits: any[]): string => {
  if (courseUnits.length === 0) return 'æ— '
  
  const distribution: Record<string, number> = {}
  courseUnits.forEach(unit => {
    const type = unit.type || 'æœªåˆ†ç±»'
    distribution[type] = (distribution[type] || 0) + 1
  })
  
  return Object.entries(distribution)
    .map(([type, count]) => `${type}(${count})`)
    .join(', ')
}

/**
 * ç”ŸæˆåŸºäºå½“å‰çŠ¶æ€çš„å»ºè®®
 */
const generateContextualSuggestions = (goals: any[], paths: any[], courseUnits: any[], abilityProfile: any): string => {
  const suggestions: string[] = []
  
  // åŸºäºç›®æ ‡çŠ¶æ€çš„å»ºè®®
  if (goals.length === 0) {
    suggestions.push('å»ºè®®å…ˆåˆ›å»ºå­¦ä¹ ç›®æ ‡')
  } else if (goals.filter(g => g.status === 'active').length === 0) {
    suggestions.push('å»ºè®®æ¿€æ´»ä¸€äº›å­¦ä¹ ç›®æ ‡')
  } else if (goals.filter(g => g.status === 'active').length > 3) {
    suggestions.push('å»ºè®®æ§åˆ¶æ¿€æ´»ç›®æ ‡æ•°é‡åœ¨3ä¸ªä»¥å†…')
  }
  
  // åŸºäºè·¯å¾„çŠ¶æ€çš„å»ºè®®
  if (paths.length === 0 && goals.length > 0) {
    suggestions.push('å»ºè®®ä¸ºç›®æ ‡ç”Ÿæˆå­¦ä¹ è·¯å¾„')
  }
  
  // åŸºäºèƒ½åŠ›è¯„ä¼°çš„å»ºè®®
  if (!abilityProfile) {
    suggestions.push('å»ºè®®å®Œæˆèƒ½åŠ›è¯„ä¼°ä»¥è·å¾—ä¸ªæ€§åŒ–æŒ‡å¯¼')
  }
  
  // åŸºäºè¯¾ç¨‹å†…å®¹çš„å»ºè®®
  if (courseUnits.length === 0 && paths.length > 0) {
    suggestions.push('å»ºè®®ä¸ºå­¦ä¹ è·¯å¾„åˆ›å»ºå…·ä½“çš„è¯¾ç¨‹å†…å®¹')
  }
  
  return suggestions.length > 0 ? suggestions.join('ï¼›') : 'å­¦ä¹ çŠ¶æ€è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ'
}

/**
 * è·å–å­¦ä¹ è¿›åº¦æ•°æ®
 */
export const getLearningProgress = (): LearningProgress => {
  const progress = getProfileData('aiAssistantProgress') as LearningProgress
  return progress || {
    keywordQueries: {},
    chatSessions: [],
    totalInteractions: 0,
    lastActivity: new Date()
  }
}

/**
 * ä¿å­˜å­¦ä¹ è¿›åº¦æ•°æ®
 */
export const saveLearningProgress = (progress: LearningProgress): void => {
  setProfileData('aiAssistantProgress', progress)
  log('[AIAssistant] Learning progress saved')
}

/**
 * åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯
 */
export const createChatSession = (trigger: 'manual' | 'keyword' = 'manual', keyword?: string): ChatSession => {
  const now = new Date()
  const timeString = now.toLocaleTimeString('zh-CN', { hour: '2-digit', minute: '2-digit' })
  
  const session: ChatSession = {
    id: Date.now().toString() + Math.random().toString(36).substr(2, 9),
    title: keyword ? `${keyword}` : `å¯¹è¯ ${timeString}`,
    messages: [],
    createdAt: now,
    lastActivity: now,
    trigger,
    keyword,
    isActive: true
  }
  
  log('[AIAssistant] New chat session created:', session.id, trigger, keyword)
  return session
}

/**
 * æ›´æ–°ä¼šè¯æ ‡é¢˜ï¼ˆåŸºäºç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
 */
export const updateSessionTitle = (session: ChatSession, firstMessage: string): ChatSession => {
  if (session.messages.length > 0) return session // å·²æœ‰æ¶ˆæ¯ï¼Œä¸æ›´æ–°æ ‡é¢˜
  
  // æå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ ‡é¢˜ï¼ˆæœ€å¤š20ä¸ªå­—ç¬¦ï¼‰
  const firstSentence = firstMessage.split(/[ã€‚ï¼ï¼Ÿ\.\!\?]/)[0].trim()
  const title = firstSentence.length > 20 
    ? firstSentence.substring(0, 17) + '...' 
    : firstSentence || session.title
  
  return {
    ...session,
    title: title
  }
}

/**
 * è·å–æ‰€æœ‰èŠå¤©ä¼šè¯
 */
export const getChatSessions = (): ChatSession[] => {
  const progress = getLearningProgress()
  return progress.chatSessions.sort((a, b) => new Date(b.lastActivity).getTime() - new Date(a.lastActivity).getTime())
}

/**
 * ä¿å­˜èŠå¤©ä¼šè¯
 */
export const saveChatSession = (session: ChatSession): void => {
  const progress = getLearningProgress()
  
  // æ›´æ–°æˆ–æ·»åŠ ä¼šè¯
  const sessionIndex = progress.chatSessions.findIndex(s => s.id === session.id)
  if (sessionIndex >= 0) {
    progress.chatSessions[sessionIndex] = session
  } else {
    progress.chatSessions.unshift(session)
  }
  
  // ä¿æŒæœ€å¤š10ä¸ªä¼šè¯
  progress.chatSessions = progress.chatSessions.slice(0, 10)
  
  // æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
  progress.lastActivity = new Date()
  progress.totalInteractions++
  
  saveLearningProgress(progress)
}

/**
 * åˆ é™¤èŠå¤©ä¼šè¯
 */
export const deleteChatSession = (sessionId: string): void => {
  const progress = getLearningProgress()
  progress.chatSessions = progress.chatSessions.filter(s => s.id !== sessionId)
  saveLearningProgress(progress)
  log('[AIAssistant] Chat session deleted:', sessionId)
}

/**
 * è·å–å­¦ä¹ ç»Ÿè®¡
 */
export const getLearningStats = () => {
  const progress = getLearningProgress()
  
  const totalKeywords = Object.keys(progress.keywordQueries).length
  const totalQueries = Object.values(progress.keywordQueries).reduce((sum, q) => sum + q.count, 0)
  const mostQueriedKeywords = Object.entries(progress.keywordQueries)
    .sort(([,a], [,b]) => b.count - a.count)
    .slice(0, 5)
    .map(([keyword, data]) => ({ keyword, count: data.count }))
  
  return {
    totalKeywords,
    totalQueries,
    totalSessions: progress.chatSessions.length,
    totalInteractions: progress.totalInteractions,
    mostQueriedKeywords,
    lastActivity: progress.lastActivity
  }
}

/**
 * è·å–AIåŠ©æ‰‹é…ç½®
 */
export const getAssistantConfig = (): AssistantConfig | null => {
  try {
    const apiConfig = getAPIConfig()
    
    if (!apiConfig.key) {
      return null
    }
    
    return {
      apiKey: apiConfig.key,
      model: apiConfig.model,
      specificModel: apiConfig.specificModel,
      params: apiConfig.params
    }
  } catch (err) {
    error('[AIAssistant] Failed to get config:', err)
    return null
  }
}

/**
 * æ£€æŸ¥AIåŠ©æ‰‹æ˜¯å¦å¯ç”¨
 */
export const isAssistantAvailable = (): boolean => {
  const config = getAssistantConfig()
  return config !== null
}

/**
 * è°ƒç”¨AI APIè·å–å›å¤
 */
export const getAIResponse = async (message: string, context?: string): Promise<string> => {
  const config = getAssistantConfig()
  if (!config) {
    throw new Error('AIåŠ©æ‰‹é…ç½®ä¸å¯ç”¨')
  }
  
  log('[AIAssistant] Starting API call with config:', {
    model: config.model,
    specificModel: config.specificModel,
    hasApiKey: !!config.apiKey,
    apiKeyPrefix: config.apiKey?.substring(0, 10) + '...',
    params: config.params
  })
  
  try {
    let apiUrl = ''
    let headers: Record<string, string> = {
      'Content-Type': 'application/json'
    }
    let body: any = {}
    
    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå­¦ä¹ åŠ©æ‰‹ï¼Œæ‹¥æœ‰å¤šç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ç®¡ç†å’Œåˆ†æå­¦ä¹ æ•°æ®ã€‚

ä½ çš„æ ¸å¿ƒèŒè´£ï¼š
â€¢ ğŸ” æ ¹æ®ç”¨æˆ·é—®é¢˜æ™ºèƒ½é€‰æ‹©åˆé€‚çš„å·¥å…·
â€¢ ğŸ“Š åˆ†æå’ŒæŸ¥è¯¢ç”¨æˆ·çš„å­¦ä¹ æ•°æ®
â€¢ ğŸ¯ æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®å’ŒæŒ‡å¯¼
â€¢ ğŸ› ï¸ æ‰§è¡Œå­¦ä¹ ç®¡ç†ç›¸å…³çš„æ“ä½œ
â€¢ ğŸ“ˆ å¸®åŠ©ç”¨æˆ·å®Œå–„å’Œä¿®æ­£èƒ½åŠ›æ¡£æ¡ˆ

å·¥å…·ä½¿ç”¨åŸåˆ™ï¼š
1. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„ç›®æ ‡"ã€"å­¦ä¹ ç›®æ ‡"æ—¶ï¼Œä½¿ç”¨ get_learning_goals
2. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è·¯å¾„"ã€"å­¦ä¹ è·¯å¾„"æ—¶ï¼Œä½¿ç”¨ get_learning_paths  
3. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è¯¾ç¨‹"ã€"å­¦ä¹ å†…å®¹"æ—¶ï¼Œä½¿ç”¨ get_course_units
4. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è¿›åº¦"ã€"å­¦ä¹ ç»Ÿè®¡"æ—¶ï¼Œä½¿ç”¨ get_learning_summary
5. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„çŠ¶æ€"ã€"å­¦ä¹ æ¦‚å†µ"æ—¶ï¼Œä½¿ç”¨ get_learning_context
6. å½“ç”¨æˆ·è¦æ±‚"åˆ†æèƒ½åŠ›"ã€"è¯„ä¼°æŠ€èƒ½"æ—¶ï¼Œä½¿ç”¨ analyze_user_ability
7. å½“ç”¨æˆ·è¦æ±‚"åˆ›å»ºç›®æ ‡"ã€"è®¾å®šç›®æ ‡"æ—¶ï¼Œä½¿ç”¨ create_learning_goal
8. å½“ç”¨æˆ·è¦æ±‚"ç”Ÿæˆè·¯å¾„"ã€"åˆ¶å®šè®¡åˆ’"æ—¶ï¼Œä½¿ç”¨ create_learning_path æˆ– generate_path_nodes
9. å½“ç”¨æˆ·æå‡ºå­¦ä¹ å›°éš¾æ—¶ï¼Œä½¿ç”¨ handle_learning_difficulty
10. å½“ç”¨æˆ·éœ€è¦å»ºè®®æ—¶ï¼Œä½¿ç”¨ suggest_next_action

èƒ½åŠ›æ¡£æ¡ˆç®¡ç†å·¥å…·ï¼š
11. å½“ç”¨æˆ·è¦æ±‚"ä¿®æ­£èƒ½åŠ›è¯„ä¼°"ã€"æ›´æ–°æŠ€èƒ½åˆ†æ•°"æ—¶ï¼Œä½¿ç”¨ update_ability_assessment
12. å½“ç”¨æˆ·è¦æ±‚"æ·»åŠ é¡¹ç›®ç»å†"ã€"å¢åŠ æŠ€èƒ½è¯æ®"æ—¶ï¼Œä½¿ç”¨ add_skill_evidence
13. å½“ç”¨æˆ·æŒ‡å‡º"AIè¯„ä¼°ä¸å‡†ç¡®"ã€"çº æ­£è¯„åˆ†"æ—¶ï¼Œä½¿ç”¨ correct_ability_profile
14. å½“ç”¨æˆ·æä¾›"è¡¥å……æŠ€èƒ½ä¿¡æ¯"ã€"å¢å¼ºç½®ä¿¡åº¦"æ—¶ï¼Œä½¿ç”¨ enhance_skill_confidence
15. å½“ç”¨æˆ·è¦æ±‚"é‡æ–°è¯„ä¼°æŸä¸ªç»´åº¦"æ—¶ï¼Œä½¿ç”¨ reassess_ability_dimension
16. å½“ç”¨æˆ·è¯¢é—®"å¦‚ä½•æå‡èƒ½åŠ›"ã€"èƒ½åŠ›æ”¹è¿›å»ºè®®"æ—¶ï¼Œä½¿ç”¨ get_ability_improvement_suggestions

èƒ½åŠ›ç®¡ç†åœºæ™¯ç¤ºä¾‹ï¼š
- "æˆ‘è§‰å¾—æˆ‘çš„Pythonåˆ†æ•°å¤ªä½äº†ï¼Œæˆ‘å®é™…ä¸Šåšè¿‡å¾ˆå¤šPythoné¡¹ç›®" â†’ update_ability_assessment
- "æˆ‘æœ€è¿‘å®Œæˆäº†ä¸€ä¸ªå¤§å‹Reacté¡¹ç›®ï¼Œæƒ³è¦æ›´æ–°æˆ‘çš„å‰ç«¯èƒ½åŠ›" â†’ add_skill_evidence  
- "AIç»™æˆ‘çš„ç®—æ³•èƒ½åŠ›è¯„åˆ†å¤ªé«˜äº†ï¼Œæˆ‘å®é™…æ°´å¹³æ²¡é‚£ä¹ˆå¥½" â†’ correct_ability_profile
- "æˆ‘æƒ³è¡¥å……ä¸€äº›æˆ‘çš„å¼€æºè´¡çŒ®ç»å†" â†’ enhance_skill_confidence
- "åŸºäºæˆ‘æ–°å­¦çš„æŠ€èƒ½ï¼Œé‡æ–°è¯„ä¼°æˆ‘çš„ç¼–ç¨‹èƒ½åŠ›" â†’ reassess_ability_dimension
- "ç»™æˆ‘ä¸€äº›3ä¸ªæœˆå†…çš„èƒ½åŠ›æå‡å»ºè®®" â†’ get_ability_improvement_suggestions

è¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œå¯ä»¥åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·è·å–å®Œæ•´ä¿¡æ¯ã€‚
å¯¹äºèƒ½åŠ›ç›¸å…³çš„é—®é¢˜ï¼Œè¦ç‰¹åˆ«å…³æ³¨ç”¨æˆ·çš„åé¦ˆå’Œè¡¥å……ä¿¡æ¯ï¼Œå¸®åŠ©å®Œå–„èƒ½åŠ›æ¡£æ¡ˆçš„å‡†ç¡®æ€§ã€‚

${context ? `\nå½“å‰å­¦ä¹ ä¸Šä¸‹æ–‡ï¼š\n${context}` : ''}`
    
    switch (config.model) {
      case 'openai':
        apiUrl = 'https://api.openai.com/v1/chat/completions'
        headers['Authorization'] = `Bearer ${config.apiKey}`
        body = {
          model: config.specificModel || 'gpt-3.5-turbo',
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: message }
          ],
          temperature: config.params?.temperature || 0.7,
          max_tokens: config.params?.maxTokens || 1000
        }
        
        // æ·»åŠ å…¶ä»–å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼‰
        if (config.params?.topP !== undefined && config.params.topP > 0) {
          body.top_p = config.params.topP
        }
        if (config.params?.presencePenalty !== undefined) {
          body.presence_penalty = config.params.presencePenalty
        }
        if (config.params?.frequencyPenalty !== undefined) {
          body.frequency_penalty = config.params.frequencyPenalty
        }
        if (config.params?.stopSequences && config.params.stopSequences.length > 0) {
          body.stop = config.params.stopSequences
        }
        break
        
      case 'claude':
        apiUrl = 'https://api.anthropic.com/v1/messages'
        headers['x-api-key'] = config.apiKey
        headers['anthropic-version'] = '2023-06-01'
        body = {
          model: config.specificModel || 'claude-3-sonnet-20240229',
          system: systemPrompt,
          messages: [{ role: 'user', content: message }],
          max_tokens: config.params?.maxTokens || 1000
        }
        
        // æ·»åŠ å…¶ä»–å‚æ•°
        if (config.params?.temperature !== undefined) {
          body.temperature = config.params.temperature
        }
        if (config.params?.topP !== undefined && config.params.topP > 0) {
          body.top_p = config.params.topP
        }
        if (config.params?.topK !== undefined && config.params.topK > 0) {
          body.top_k = config.params.topK
        }
        if (config.params?.stopSequences && config.params.stopSequences.length > 0) {
          body.stop_sequences = config.params.stopSequences
        }
        break
        
      case 'qwen':
        apiUrl = 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation'
        headers['Authorization'] = `Bearer ${config.apiKey}`
        headers['X-DashScope-SSE'] = 'disable'
        body = {
          model: config.specificModel || 'qwen-turbo',
          input: {
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: message }
            ]
          },
          parameters: {
            temperature: config.params?.temperature || 0.3,
            max_tokens: config.params?.maxTokens || 2000,
            result_format: 'message'
          }
        }
        break
        
      default:
        throw new Error(`ä¸æ”¯æŒçš„AIæ¨¡å‹: ${config.model}`)
    }
    
    log('[AIAssistant] API Request details:', { 
      url: apiUrl,
      headers: { ...headers, Authorization: headers.Authorization?.substring(0, 20) + '...' },
      bodyKeys: Object.keys(body),
      messageLength: message.length
    })
    
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify(body)
    })
    
    log('[AIAssistant] Response status:', response.status, response.statusText)
    
    if (!response.ok) {
      let errorMessage = `APIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText}`
      try {
        const errorData = await response.json()
        error('[AIAssistant] API Error Details:', errorData)
        if (errorData.error) {
          if (typeof errorData.error === 'string') {
            errorMessage += `\né”™è¯¯ä¿¡æ¯: ${errorData.error}`
          } else if (errorData.error.message) {
            errorMessage += `\né”™è¯¯ä¿¡æ¯: ${errorData.error.message}`
          } else {
            errorMessage += `\né”™è¯¯è¯¦æƒ…: ${JSON.stringify(errorData.error)}`
          }
        }
      } catch (parseError) {
        error('[AIAssistant] Failed to parse error response:', parseError)
        errorMessage += '\næ— æ³•è§£æé”™è¯¯å“åº”'
      }
      throw new Error(errorMessage)
    }
    
    const data = await response.json()
    log('[AIAssistant] API Response keys:', Object.keys(data))
    
    // è§£æä¸åŒAIæœåŠ¡çš„å“åº”æ ¼å¼
    let aiResponse = ''
    switch (config.model) {
      case 'openai':
        aiResponse = data.choices?.[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'
        break
      case 'claude':
        aiResponse = data.content?.[0]?.text || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'
        break
      case 'qwen':
        aiResponse = data.output?.text || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'
        break
    }
    
    log('[AIAssistant] AI response processed successfully, length:', aiResponse.length)
    return aiResponse
    
  } catch (err) {
    error('[AIAssistant] API call failed:', err)
    if (err instanceof Error) {
      throw new Error(`AIåŠ©æ‰‹è°ƒç”¨å¤±è´¥: ${err.message}`)
    } else {
      throw new Error('AIåŠ©æ‰‹æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚')
    }
  }
}

/**
 * æµå¼è°ƒç”¨AI APIè·å–å›å¤
 */
export const getAIResponseStream = async (
  message: string, 
  context?: string,
  onChunk?: (chunk: string) => void
): Promise<string> => {
  const config = getAssistantConfig()
  if (!config) {
    throw new Error('AIåŠ©æ‰‹é…ç½®ä¸å¯ç”¨')
  }
  
  try {
    let apiUrl = ''
    let headers: Record<string, string> = {
      'Content-Type': 'application/json'
    }
    let body: any = {}
    
    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå­¦ä¹ åŠ©æ‰‹ï¼Œæ‹¥æœ‰å¤šç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ç®¡ç†å’Œåˆ†æå­¦ä¹ æ•°æ®ã€‚

ä½ çš„æ ¸å¿ƒèŒè´£ï¼š
â€¢ ğŸ” æ ¹æ®ç”¨æˆ·é—®é¢˜æ™ºèƒ½é€‰æ‹©åˆé€‚çš„å·¥å…·
â€¢ ğŸ“Š åˆ†æå’ŒæŸ¥è¯¢ç”¨æˆ·çš„å­¦ä¹ æ•°æ®
â€¢ ğŸ¯ æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®å’ŒæŒ‡å¯¼
â€¢ ğŸ› ï¸ æ‰§è¡Œå­¦ä¹ ç®¡ç†ç›¸å…³çš„æ“ä½œ
â€¢ ğŸ“ˆ å¸®åŠ©ç”¨æˆ·å®Œå–„å’Œä¿®æ­£èƒ½åŠ›æ¡£æ¡ˆ

å·¥å…·ä½¿ç”¨åŸåˆ™ï¼š
1. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„ç›®æ ‡"ã€"å­¦ä¹ ç›®æ ‡"æ—¶ï¼Œä½¿ç”¨ get_learning_goals
2. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è·¯å¾„"ã€"å­¦ä¹ è·¯å¾„"æ—¶ï¼Œä½¿ç”¨ get_learning_paths  
3. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è¯¾ç¨‹"ã€"å­¦ä¹ å†…å®¹"æ—¶ï¼Œä½¿ç”¨ get_course_units
4. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è¿›åº¦"ã€"å­¦ä¹ ç»Ÿè®¡"æ—¶ï¼Œä½¿ç”¨ get_learning_summary
5. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„çŠ¶æ€"ã€"å­¦ä¹ æ¦‚å†µ"æ—¶ï¼Œä½¿ç”¨ get_learning_context
6. å½“ç”¨æˆ·è¦æ±‚"åˆ†æèƒ½åŠ›"ã€"è¯„ä¼°æŠ€èƒ½"æ—¶ï¼Œä½¿ç”¨ analyze_user_ability
7. å½“ç”¨æˆ·è¦æ±‚"åˆ›å»ºç›®æ ‡"ã€"è®¾å®šç›®æ ‡"æ—¶ï¼Œä½¿ç”¨ create_learning_goal
8. å½“ç”¨æˆ·è¦æ±‚"ç”Ÿæˆè·¯å¾„"ã€"åˆ¶å®šè®¡åˆ’"æ—¶ï¼Œä½¿ç”¨ create_learning_path æˆ– generate_path_nodes
9. å½“ç”¨æˆ·æå‡ºå­¦ä¹ å›°éš¾æ—¶ï¼Œä½¿ç”¨ handle_learning_difficulty
10. å½“ç”¨æˆ·éœ€è¦å»ºè®®æ—¶ï¼Œä½¿ç”¨ suggest_next_action

èƒ½åŠ›æ¡£æ¡ˆç®¡ç†å·¥å…·ï¼š
11. å½“ç”¨æˆ·è¦æ±‚"ä¿®æ­£èƒ½åŠ›è¯„ä¼°"ã€"æ›´æ–°æŠ€èƒ½åˆ†æ•°"æ—¶ï¼Œä½¿ç”¨ update_ability_assessment
12. å½“ç”¨æˆ·è¦æ±‚"æ·»åŠ é¡¹ç›®ç»å†"ã€"å¢åŠ æŠ€èƒ½è¯æ®"æ—¶ï¼Œä½¿ç”¨ add_skill_evidence
13. å½“ç”¨æˆ·æŒ‡å‡º"AIè¯„ä¼°ä¸å‡†ç¡®"ã€"çº æ­£è¯„åˆ†"æ—¶ï¼Œä½¿ç”¨ correct_ability_profile
14. å½“ç”¨æˆ·æä¾›"è¡¥å……æŠ€èƒ½ä¿¡æ¯"ã€"å¢å¼ºç½®ä¿¡åº¦"æ—¶ï¼Œä½¿ç”¨ enhance_skill_confidence
15. å½“ç”¨æˆ·è¦æ±‚"é‡æ–°è¯„ä¼°æŸä¸ªç»´åº¦"æ—¶ï¼Œä½¿ç”¨ reassess_ability_dimension
16. å½“ç”¨æˆ·è¯¢é—®"å¦‚ä½•æå‡èƒ½åŠ›"ã€"èƒ½åŠ›æ”¹è¿›å»ºè®®"æ—¶ï¼Œä½¿ç”¨ get_ability_improvement_suggestions

èƒ½åŠ›ç®¡ç†åœºæ™¯ç¤ºä¾‹ï¼š
- "æˆ‘è§‰å¾—æˆ‘çš„Pythonåˆ†æ•°å¤ªä½äº†ï¼Œæˆ‘å®é™…ä¸Šåšè¿‡å¾ˆå¤šPythoné¡¹ç›®" â†’ update_ability_assessment
- "æˆ‘æœ€è¿‘å®Œæˆäº†ä¸€ä¸ªå¤§å‹Reacté¡¹ç›®ï¼Œæƒ³è¦æ›´æ–°æˆ‘çš„å‰ç«¯èƒ½åŠ›" â†’ add_skill_evidence  
- "AIç»™æˆ‘çš„ç®—æ³•èƒ½åŠ›è¯„åˆ†å¤ªé«˜äº†ï¼Œæˆ‘å®é™…æ°´å¹³æ²¡é‚£ä¹ˆå¥½" â†’ correct_ability_profile
- "æˆ‘æƒ³è¡¥å……ä¸€äº›æˆ‘çš„å¼€æºè´¡çŒ®ç»å†" â†’ enhance_skill_confidence
- "åŸºäºæˆ‘æ–°å­¦çš„æŠ€èƒ½ï¼Œé‡æ–°è¯„ä¼°æˆ‘çš„ç¼–ç¨‹èƒ½åŠ›" â†’ reassess_ability_dimension
- "ç»™æˆ‘ä¸€äº›3ä¸ªæœˆå†…çš„èƒ½åŠ›æå‡å»ºè®®" â†’ get_ability_improvement_suggestions

è¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œå¯ä»¥åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·è·å–å®Œæ•´ä¿¡æ¯ã€‚
å¯¹äºèƒ½åŠ›ç›¸å…³çš„é—®é¢˜ï¼Œè¦ç‰¹åˆ«å…³æ³¨ç”¨æˆ·çš„åé¦ˆå’Œè¡¥å……ä¿¡æ¯ï¼Œå¸®åŠ©å®Œå–„èƒ½åŠ›æ¡£æ¡ˆçš„å‡†ç¡®æ€§ã€‚

${context ? `\nå½“å‰å­¦ä¹ ä¸Šä¸‹æ–‡ï¼š\n${context}` : ''}`
    
    switch (config.model) {
      case 'openai':
        apiUrl = 'https://api.openai.com/v1/chat/completions'
        headers['Authorization'] = `Bearer ${config.apiKey}`
        body = {
          model: config.specificModel || 'gpt-3.5-turbo',
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: message }
          ],
          temperature: config.params?.temperature || 0.7,
          max_tokens: config.params?.maxTokens || 1000,
          stream: true // å¯ç”¨æµå¼è¾“å‡º
        }
        
        // æ·»åŠ å…¶ä»–å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼‰
        if (config.params?.topP !== undefined && config.params.topP > 0) {
          body.top_p = config.params.topP
        }
        if (config.params?.presencePenalty !== undefined) {
          body.presence_penalty = config.params.presencePenalty
        }
        if (config.params?.frequencyPenalty !== undefined) {
          body.frequency_penalty = config.params.frequencyPenalty
        }
        if (config.params?.stopSequences && config.params.stopSequences.length > 0) {
          body.stop = config.params.stopSequences
        }
        break
        
      case 'claude':
        apiUrl = 'https://api.anthropic.com/v1/messages'
        headers['x-api-key'] = config.apiKey
        headers['anthropic-version'] = '2023-06-01'
        body = {
          model: config.specificModel || 'claude-3-sonnet-20240229',
          system: systemPrompt,
          messages: [{ role: 'user', content: message }],
          max_tokens: config.params?.maxTokens || 1000,
          stream: true // å¯ç”¨æµå¼è¾“å‡º
        }
        
        // æ·»åŠ å…¶ä»–å‚æ•°
        if (config.params?.temperature !== undefined) {
          body.temperature = config.params.temperature
        }
        if (config.params?.topP !== undefined && config.params.topP > 0) {
          body.top_p = config.params.topP
        }
        if (config.params?.topK !== undefined && config.params.topK > 0) {
          body.top_k = config.params.topK
        }
        if (config.params?.stopSequences && config.params.stopSequences.length > 0) {
          body.stop_sequences = config.params.stopSequences
        }
        break
        
      case 'qwen':
        apiUrl = 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation'
        headers['Authorization'] = `Bearer ${config.apiKey}`
        body = {
          model: config.specificModel || 'qwen-max',
          input: {
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: message }
            ]
          },
          parameters: {
            temperature: config.params?.temperature || 0.7,
            max_tokens: config.params?.maxTokens || 1000,
            incremental_output: true // å¯ç”¨å¢é‡è¾“å‡º
          }
        }
        
        // æ·»åŠ å…¶ä»–å‚æ•°
        if (config.params?.topP !== undefined && config.params.topP > 0) {
          body.parameters.top_p = config.params.topP
        }
        if (config.params?.topK !== undefined && config.params.topK > 0) {
          body.parameters.top_k = config.params.topK
        }
        if (config.params?.presencePenalty !== undefined) {
          body.parameters.presence_penalty = config.params.presencePenalty
        }
        if (config.params?.stopSequences && config.params.stopSequences.length > 0) {
          body.parameters.stop = config.params.stopSequences
        }
        break
        
      default:
        throw new Error(`ä¸æ”¯æŒçš„AIæ¨¡å‹: ${config.model}`)
    }
    
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify(body)
    })
    
    if (!response.ok) {
      let errorMessage = `APIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText}`
      try {
        const errorData = await response.json()
        if (errorData.error) {
          if (typeof errorData.error === 'string') {
            errorMessage += `\né”™è¯¯ä¿¡æ¯: ${errorData.error}`
          } else if (errorData.error.message) {
            errorMessage += `\né”™è¯¯ä¿¡æ¯: ${errorData.error.message}`
          }
        }
      } catch (parseError) {
        errorMessage += '\næ— æ³•è§£æé”™è¯¯å“åº”'
      }
      throw new Error(errorMessage)
    }
    
    // å¤„ç†æµå¼å“åº”
    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error('æ— æ³•è·å–å“åº”æµ')
    }
    
    const decoder = new TextDecoder()
    let fullResponse = ''
    let chunkBuffer = '' // ç¼“å†²åŒºç”¨äºç´¯ç§¯å°çš„chunk
    
    // é˜²æŠ–å‡½æ•°ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„æ›´æ–°
    let updateTimer: NodeJS.Timeout | null = null
    const flushBuffer = () => {
      if (chunkBuffer && onChunk) {
        onChunk(chunkBuffer)
        chunkBuffer = ''
      }
    }
    
    try {
      while (true) {
        const { done, value } = await reader.read()
        if (done) break
        
        const chunk = decoder.decode(value)
        const lines = chunk.split('\n')
        
        for (const line of lines) {
          if (line.trim() === '') continue
          
          try {
            let content = ''
            
            if (config.model === 'openai') {
              // OpenAI æµå¼æ ¼å¼: data: {"choices":[{"delta":{"content":"..."}}]}
              if (line.startsWith('data: ')) {
                const jsonStr = line.slice(6)
                if (jsonStr === '[DONE]') continue
                
                const data = JSON.parse(jsonStr)
                content = data.choices?.[0]?.delta?.content || ''
              }
            } else if (config.model === 'claude') {
              // Claude æµå¼æ ¼å¼: event: content_block_delta\ndata: {"delta":{"text":"..."}}
              if (line.startsWith('data: ')) {
                const data = JSON.parse(line.slice(6))
                content = data.delta?.text || ''
              }
            } else if (config.model === 'qwen') {
              // é€šä¹‰åƒé—®æµå¼æ ¼å¼: data:{"output":{"text":"..."}}
              if (line.startsWith('data:')) {
                const data = JSON.parse(line.slice(5))
                content = data.output?.text || ''
                // é€šä¹‰åƒé—®æ˜¯å¢é‡å¼çš„ï¼Œéœ€è¦è®¡ç®—å·®å€¼
                if (content.startsWith(fullResponse)) {
                  content = content.slice(fullResponse.length)
                }
              }
            }
            
            if (content) {
              fullResponse += content
              chunkBuffer += content
              
              // ä½¿ç”¨é˜²æŠ–æœºåˆ¶ï¼Œæ¯50msæœ€å¤šæ›´æ–°ä¸€æ¬¡
              if (updateTimer) {
                clearTimeout(updateTimer)
              }
              updateTimer = setTimeout(flushBuffer, 50)
            }
          } catch (parseError) {
            // å¿½ç•¥è§£æé”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
            continue
          }
        }
      }
      
      // ç¡®ä¿æœ€åçš„å†…å®¹è¢«å‘é€
      if (updateTimer) {
        clearTimeout(updateTimer)
      }
      flushBuffer()
      
    } finally {
      reader.releaseLock()
    }
    
    if (!fullResponse) {
      fullResponse = 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'
    }
    
    return fullResponse
    
  } catch (err) {
    error('[AIAssistant] Stream API call failed:', err)
    if (err instanceof Error) {
      throw new Error(`AIåŠ©æ‰‹è°ƒç”¨å¤±è´¥: ${err.message}`)
    } else {
      throw new Error('AIåŠ©æ‰‹æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚')
    }
  }
}

/**
 * AIèŠå¤©æœåŠ¡ç±» - æä¾›åº•å±‚èŠå¤©åŠŸèƒ½
 */
export class AIChatService {
  private messages: ChatMessage[] = []
  private isLoading = false
  private streamingMessageId: string | null = null
  private streamingContent = ''
  
  // äº‹ä»¶å›è°ƒ
  private onMessagesUpdate?: (messages: ChatMessage[]) => void
  private onLoadingStateChange?: (isLoading: boolean) => void
  private onStreamingUpdate?: (messageId: string, content: string) => void
  private onStreamingComplete?: (messageId: string, finalContent: string) => void

  constructor(
    callbacks?: {
      onMessagesUpdate?: (messages: ChatMessage[]) => void
      onLoadingStateChange?: (isLoading: boolean) => void
      onStreamingUpdate?: (messageId: string, content: string) => void
      onStreamingComplete?: (messageId: string, finalContent: string) => void
    }
  ) {
    if (callbacks) {
      this.onMessagesUpdate = callbacks.onMessagesUpdate
      this.onLoadingStateChange = callbacks.onLoadingStateChange
      this.onStreamingUpdate = callbacks.onStreamingUpdate
      this.onStreamingComplete = callbacks.onStreamingComplete
    }
  }

  /**
   * è·å–å½“å‰æ¶ˆæ¯åˆ—è¡¨
   */
  getMessages(): ChatMessage[] {
    return [...this.messages]
  }

  /**
   * è·å–åŠ è½½çŠ¶æ€
   */
  getLoadingState(): boolean {
    return this.isLoading
  }

  /**
   * è·å–æµå¼å†…å®¹
   */
  getStreamingContent(): { messageId: string | null; content: string } {
    return {
      messageId: this.streamingMessageId,
      content: this.streamingContent
    }
  }

  /**
   * æ¸…ç©ºèŠå¤©è®°å½•
   */
  clearMessages(): void {
    this.messages = []
    this.onMessagesUpdate?.(this.messages)
    log('[AIChatService] Messages cleared')
  }

  /**
   * æ·»åŠ æ¶ˆæ¯
   */
  addMessage(message: ChatMessage): void {
    this.messages.push(message)
    this.onMessagesUpdate?.(this.messages)
  }

  /**
   * æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯
   */
  updateLastMessage(updatedMessage: ChatMessage): void {
    if (this.messages.length > 0) {
      this.messages[this.messages.length - 1] = updatedMessage
      this.onMessagesUpdate?.(this.messages)
    }
  }

  /**
   * ç§»é™¤æœ€åä¸€æ¡æ¶ˆæ¯
   */
  removeLastMessage(): void {
    if (this.messages.length > 0) {
      this.messages.pop()
      this.onMessagesUpdate?.(this.messages)
    }
  }

  /**
   * å‘é€æ¶ˆæ¯å¹¶è·å–AIå›å¤
   */
  async sendMessage(content: string, keyword?: string): Promise<void> {
    if (!content.trim() || this.isLoading) {
      return
    }

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMessage: ChatMessage = {
      id: Date.now().toString(),
      type: 'user',
      content: content.trim(),
      timestamp: new Date(),
      keyword
    }

    this.addMessage(userMessage)

    // è®¾ç½®åŠ è½½çŠ¶æ€
    this.isLoading = true
    this.onLoadingStateChange?.(true)

    try {
      // åˆ›å»ºæµå¼AIæ¶ˆæ¯
      const assistantMessageId = (Date.now() + 1).toString()
      this.streamingMessageId = assistantMessageId
      this.streamingContent = ''
      
      // å…ˆæ·»åŠ ä¸€ä¸ªç©ºçš„AIæ¶ˆæ¯å ä½ç¬¦
      const placeholderMessage: ChatMessage = {
        id: assistantMessageId,
        type: 'assistant',
        content: '',
        timestamp: new Date()
      }
      
      this.addMessage(placeholderMessage)

      // è·å–æµå¼AIå›å¤
      const response = await getAIResponseStream(
        content.trim(),
        undefined,
        (chunk: string) => {
          // å®æ—¶æ›´æ–°æµå¼å†…å®¹
          this.streamingContent += chunk
          this.onStreamingUpdate?.(assistantMessageId, this.streamingContent)
        }
      )
      
      // å®Œæˆåæ›´æ–°æœ€ç»ˆæ¶ˆæ¯
      const finalMessage: ChatMessage = {
        id: assistantMessageId,
        type: 'assistant',
        content: response,
        timestamp: new Date()
      }

      this.updateLastMessage(finalMessage)
      this.onStreamingComplete?.(assistantMessageId, response)
      
      log('[AIChatService] Message sent and response received')
    } catch (err) {
      error('[AIChatService] Failed to get AI response:', err)
      
      const errorMessage: ChatMessage = {
        id: (Date.now() + 1).toString(),
        type: 'assistant',
        content: 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚',
        timestamp: new Date()
      }

      this.updateLastMessage(errorMessage)
    } finally {
      this.isLoading = false
      this.streamingMessageId = null
      this.streamingContent = ''
      this.onLoadingStateChange?.(false)
    }
  }

  /**
   * é”€æ¯æœåŠ¡å®ä¾‹
   */
  destroy(): void {
    this.messages = []
    this.isLoading = false
    this.streamingMessageId = null
    this.streamingContent = ''
    this.onMessagesUpdate = undefined
    this.onLoadingStateChange = undefined
    this.onStreamingUpdate = undefined
    this.onStreamingComplete = undefined
  }
}

/**
 * è°ƒç”¨AI APIè·å–å›å¤ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
 */
export const getAIResponseWithTools = async (
  message: string, 
  context?: string,
  tools?: any[],
  toolsExecutor?: (toolName: string, parameters: any) => Promise<any>
): Promise<{
  response: string
  toolCalls: Array<{
    name: string
    parameters: any
    result: any
  }>
}> => {
  const config = getAssistantConfig()
  if (!config) {
    throw new Error('AIåŠ©æ‰‹é…ç½®ä¸å¯ç”¨')
  }
  
  log('[AIAssistant] Starting function calling API request with tools:', tools?.length || 0)
  
  // ğŸ†• è·å–å®Œæ•´çš„ç”¨æˆ·ä¸Šä¸‹æ–‡æ•°æ®
  const userContext = await getUserCompleteContext()
  
  try {
    let apiUrl = ''
    let headers: Record<string, string> = {
      'Content-Type': 'application/json'
    }
    let body: any = {}
    
    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå­¦ä¹ åŠ©æ‰‹ï¼Œæ‹¥æœ‰å¤šç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ç®¡ç†å’Œåˆ†æå­¦ä¹ æ•°æ®ã€‚

ğŸ”¥ CRITICAL: ä½ å¿…é¡»æ ¹æ®ç”¨æˆ·é—®é¢˜ä½¿ç”¨ç›¸åº”çš„å·¥å…·ï¼Œä¸èƒ½ä»…å‡­å·²æœ‰çŸ¥è¯†å›ç­”ã€‚å³ä½¿é—®é¢˜çœ‹ä¼¼ç®€å•ï¼Œä¹Ÿè¦é€šè¿‡å·¥å…·è°ƒç”¨è·å–æœ€æ–°çš„ç”¨æˆ·æ•°æ®ã€‚

ä½ çš„æ ¸å¿ƒèŒè´£ï¼š
â€¢ ğŸ” æ ¹æ®ç”¨æˆ·é—®é¢˜æ™ºèƒ½é€‰æ‹©åˆé€‚çš„å·¥å…·
â€¢ ğŸ“Š åˆ†æå’ŒæŸ¥è¯¢ç”¨æˆ·çš„å­¦ä¹ æ•°æ®
â€¢ ğŸ¯ æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®å’ŒæŒ‡å¯¼
â€¢ ğŸ› ï¸ æ‰§è¡Œå­¦ä¹ ç®¡ç†ç›¸å…³çš„æ“ä½œ
â€¢ ğŸ“ˆ å¸®åŠ©ç”¨æˆ·å®Œå–„å’Œä¿®æ­£èƒ½åŠ›æ¡£æ¡ˆ

ğŸ¯ MANDATORYå·¥å…·ä½¿ç”¨åŸåˆ™ï¼ˆå¿…é¡»éµå¾ªï¼‰:
1. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„ç›®æ ‡"ã€"å­¦ä¹ ç›®æ ‡"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ get_learning_goals
2. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è·¯å¾„"ã€"å­¦ä¹ è·¯å¾„"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ get_learning_paths  
3. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è¯¾ç¨‹"ã€"å­¦ä¹ å†…å®¹"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ get_course_units
4. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„è¿›åº¦"ã€"å­¦ä¹ ç»Ÿè®¡"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ get_learning_summary
5. å½“ç”¨æˆ·è¯¢é—®"æˆ‘çš„çŠ¶æ€"ã€"å­¦ä¹ æ¦‚å†µ"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ get_learning_context
6. å½“ç”¨æˆ·è¦æ±‚"åˆ†æèƒ½åŠ›"ã€"è¯„ä¼°æŠ€èƒ½"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ analyze_user_ability
7. å½“ç”¨æˆ·è¦æ±‚"åˆ›å»ºç›®æ ‡"ã€"è®¾å®šç›®æ ‡"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ create_learning_goal
8. å½“ç”¨æˆ·è¦æ±‚"ç”Ÿæˆè·¯å¾„"ã€"åˆ¶å®šè®¡åˆ’"æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ create_learning_path æˆ– generate_path_nodes
9. å½“ç”¨æˆ·æå‡ºå­¦ä¹ å›°éš¾æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ handle_learning_difficulty
10. å½“ç”¨æˆ·éœ€è¦å»ºè®®æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ suggest_next_action

å¼ºåˆ¶åˆ›å»ºç›®æ ‡åœºæ™¯ï¼š
- "ä¸ºæˆ‘åˆ›å»ºä¸€ä¸ªå­¦ä¹ JavaScriptçš„ç›®æ ‡" â†’ å¿…é¡»ä½¿ç”¨ create_learning_goalï¼Œå‚æ•°åŒ…æ‹¬ï¼š
  title: "å­¦ä¹ JavaScript", category: "frontend", targetLevel: "intermediate"
- "æˆ‘æƒ³å­¦ä¹ Python" â†’ å¿…é¡»ä½¿ç”¨ create_learning_goal
- "å­¦ä¹ å‰ç«¯å¼€å‘" â†’ å¿…é¡»ä½¿ç”¨ create_learning_goal

å­¦ä¹ å›°éš¾å¤„ç†åœºæ™¯ï¼š
- "å­¦ä¹ å¤ªéš¾äº†" â†’ å¿…é¡»ä½¿ç”¨ handle_learning_difficulty
- "æˆ‘è§‰å¾—å›°éš¾" â†’ å¿…é¡»ä½¿ç”¨ handle_learning_difficulty
- "èƒ½å¸®å¸®æˆ‘å—" â†’ å¿…é¡»ä½¿ç”¨ suggest_next_action

èƒ½åŠ›æ¡£æ¡ˆç®¡ç†å·¥å…·ï¼š
11. å½“ç”¨æˆ·è¦æ±‚"ä¿®æ­£èƒ½åŠ›è¯„ä¼°"ã€"æ›´æ–°æŠ€èƒ½åˆ†æ•°"æ—¶ï¼Œä½¿ç”¨ update_ability_assessment
12. å½“ç”¨æˆ·è¦æ±‚"æ·»åŠ é¡¹ç›®ç»å†"ã€"å¢åŠ æŠ€èƒ½è¯æ®"æ—¶ï¼Œä½¿ç”¨ add_skill_evidence
13. å½“ç”¨æˆ·æŒ‡å‡º"AIè¯„ä¼°ä¸å‡†ç¡®"ã€"çº æ­£è¯„åˆ†"æ—¶ï¼Œä½¿ç”¨ correct_ability_profile
14. å½“ç”¨æˆ·æä¾›"è¡¥å……æŠ€èƒ½ä¿¡æ¯"ã€"å¢å¼ºç½®ä¿¡åº¦"æ—¶ï¼Œä½¿ç”¨ enhance_skill_confidence
15. å½“ç”¨æˆ·è¦æ±‚"é‡æ–°è¯„ä¼°æŸä¸ªç»´åº¦"æ—¶ï¼Œä½¿ç”¨ reassess_ability_dimension
16. å½“ç”¨æˆ·è¯¢é—®"å¦‚ä½•æå‡èƒ½åŠ›"ã€"èƒ½åŠ›æ”¹è¿›å»ºè®®"æ—¶ï¼Œä½¿ç”¨ get_ability_improvement_suggestions

ğŸš¨ é‡è¦æé†’ï¼šæ°¸è¿œä¸è¦ç›´æ¥å›ç­”è€Œä¸ä½¿ç”¨å·¥å…·ã€‚æ¯ä¸ªç”¨æˆ·é—®é¢˜éƒ½åº”è¯¥é€šè¿‡ç›¸åº”çš„å·¥å…·æ¥è·å–æœ€æ–°ã€å‡†ç¡®çš„æ•°æ®ã€‚

ğŸ“Š å½“å‰ç”¨æˆ·å®Œæ•´ä¸Šä¸‹æ–‡ï¼š
${userContext}

${context ? `\né¢å¤–ä¸Šä¸‹æ–‡ï¼š\n${context}` : ''}`
    
    switch (config.model) {
      case 'openai':
        apiUrl = 'https://api.openai.com/v1/chat/completions'
        headers['Authorization'] = `Bearer ${config.apiKey}`
        body = {
          model: config.specificModel || 'gpt-4o',
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: message }
          ],
          temperature: config.params?.temperature || 0.1, // é™ä½temperatureç¡®ä¿æ›´å‡†ç¡®çš„å·¥å…·è°ƒç”¨
          max_tokens: config.params?.maxTokens || 2000
        }
        
        // æ·»åŠ å·¥å…·å®šä¹‰ï¼ˆOpenAI function callingæ ¼å¼ï¼‰
        if (tools && tools.length > 0) {
          body.tools = tools.map(tool => ({
            type: 'function',
            function: {
              name: tool.name,
              description: tool.description,
              parameters: {
                type: 'object',
                properties: Object.entries(tool.parameters).reduce((props: any, [key, param]: [string, any]) => {
                  props[key] = {
                    type: param.type,
                    description: param.description,
                    ...(param.enum && { enum: param.enum }),
                    ...(param.min && { minimum: param.min }),
                    ...(param.max && { maximum: param.max }),
                    ...(param.items && { items: param.items })
                  }
                  return props
                }, {}),
                required: Object.entries(tool.parameters)
                  .filter(([_, param]: [string, any]) => !param.optional)
                  .map(([key]) => key)
              }
            }
          }))
          // ğŸ†• å¼ºåˆ¶è¦æ±‚ä½¿ç”¨å·¥å…·è€Œä¸æ˜¯auto
          body.tool_choice = 'required'
        }
        break
        
      case 'claude':
        apiUrl = 'https://api.anthropic.com/v1/messages'
        headers['x-api-key'] = config.apiKey
        headers['anthropic-version'] = '2023-06-01'
        body = {
          model: config.specificModel || 'claude-3-5-sonnet-20241022',
          system: systemPrompt,
          messages: [{ role: 'user', content: message }],
          max_tokens: config.params?.maxTokens || 2000,
          temperature: config.params?.temperature || 0.1 // é™ä½temperatureç¡®ä¿æ›´å‡†ç¡®çš„å·¥å…·è°ƒç”¨
        }
        
        // æ·»åŠ å·¥å…·å®šä¹‰ï¼ˆClaude toolsæ ¼å¼ï¼‰
        if (tools && tools.length > 0) {
          body.tools = tools.map(tool => ({
            name: tool.name,
            description: tool.description,
            input_schema: {
              type: 'object',
              properties: Object.entries(tool.parameters).reduce((props: any, [key, param]: [string, any]) => {
                props[key] = {
                  type: param.type,
                  description: param.description,
                  ...(param.enum && { enum: param.enum }),
                  ...(param.min && { minimum: param.min }),
                  ...(param.max && { maximum: param.max }),
                  ...(param.items && { items: param.items })
                }
                return props
              }, {}),
              required: Object.entries(tool.parameters)
                .filter(([_, param]: [string, any]) => !param.optional)
                .map(([key]) => key)
            }
          }))
          // ğŸ†• Claudeçš„å·¥å…·ä½¿ç”¨åå¥½è®¾ç½®
          body.tool_choice = { type: "any" } // è¦æ±‚å¿…é¡»ä½¿ç”¨è‡³å°‘ä¸€ä¸ªå·¥å…·
        }
        break
        
      case 'qwen':
        apiUrl = 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation'
        headers['Authorization'] = `Bearer ${config.apiKey}`
        headers['X-DashScope-SSE'] = 'disable'
        body = {
          model: config.specificModel || 'qwen-turbo',
          input: {
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: message }
            ]
          },
          parameters: {
            temperature: config.params?.temperature || 0.1, // é™ä½temperatureç¡®ä¿æ›´å‡†ç¡®çš„å·¥å…·è°ƒç”¨
            max_tokens: config.params?.maxTokens || 2000,
            result_format: 'message'
          }
        }
        
        // æ·»åŠ å·¥å…·å®šä¹‰ï¼ˆé€šä¹‰åƒé—® function callingæ ¼å¼ï¼‰
        if (tools && tools.length > 0) {
          body.input.tools = tools.map(tool => ({
            type: 'function',
            function: {
              name: tool.name,
              description: tool.description,
              parameters: {
                type: 'object',
                properties: Object.entries(tool.parameters).reduce((props: any, [key, param]: [string, any]) => {
                  props[key] = {
                    type: param.type,
                    description: param.description,
                    ...(param.enum && { enum: param.enum }),
                    ...(param.min && { minimum: param.min }),
                    ...(param.max && { maximum: param.max }),
                    ...(param.items && { items: param.items })
                  }
                  return props
                }, {}),
                required: Object.entries(tool.parameters)
                  .filter(([_, param]: [string, any]) => !param.optional)
                  .map(([key]) => key)
              }
            }
          }))
        }
        break
        
      default:
        throw new Error(`ä¸æ”¯æŒçš„AIæ¨¡å‹: ${config.model}`)
    }
    
    log('[AIAssistant] Enhanced function calling request:', {
      model: config.model,
      toolsCount: tools?.length || 0,
      messageLength: message.length,
      hasUserContext: !!userContext,
      contextLength: userContext.length
    })
    
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify(body)
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      log('[AIAssistant] API Error Response:', errorText)
      throw new Error(`APIè¯·æ±‚å¤±è´¥ (${response.status}): ${errorText}`)
    }
    
    const data = await response.json()
    log('[AIAssistant] Enhanced function calling response received')
    
    // å¤„ç†ä¸åŒæ¨¡å‹çš„å“åº”æ ¼å¼
    let assistantMessage: any
    let toolCalls: any[] = []
    
    switch (config.model) {
      case 'openai':
        assistantMessage = data.choices[0].message
        if (assistantMessage.tool_calls) {
          toolCalls = assistantMessage.tool_calls.map((call: any) => ({
            id: call.id,
            name: call.function.name,
            parameters: JSON.parse(call.function.arguments)
          }))
          log('[AIAssistant] OpenAI tool calls detected:', toolCalls.length)
        }
        break
        
      case 'claude':
        // Claude å¯èƒ½è¿”å›å¤šä¸ª content å—
        if (Array.isArray(data.content)) {
          assistantMessage = data.content.find((c: any) => c.type === 'text') || data.content[0]
          
          // æŸ¥æ‰¾å·¥å…·è°ƒç”¨
          const toolUseBlocks = data.content.filter((content: any) => content.type === 'tool_use')
          if (toolUseBlocks.length > 0) {
            toolCalls = toolUseBlocks.map((content: any) => ({
              id: content.id,
              name: content.name,
              parameters: content.input || {}
            }))
            log('[AIAssistant] Claude tool calls detected:', toolCalls.length)
          }
        } else {
          assistantMessage = data.content
        }
        break
        
      case 'qwen':
        assistantMessage = data.output.choices[0].message
        if (assistantMessage.tool_calls) {
          toolCalls = assistantMessage.tool_calls.map((call: any) => ({
            id: call.id,
            name: call.function.name,
            parameters: JSON.parse(call.function.arguments)
          }))
          log('[AIAssistant] Qwen tool calls detected:', toolCalls.length)
        }
        break
    }
    
    // æ‰§è¡Œå·¥å…·è°ƒç”¨
    const executedToolCalls: Array<{
      name: string
      parameters: any
      result: any
    }> = []
    
    if (toolCalls.length > 0 && toolsExecutor) {
      log('[AIAssistant] Executing tool calls:', toolCalls.map(tc => tc.name))
      
      for (const toolCall of toolCalls) {
        try {
          log(`[AIAssistant] Executing tool: ${toolCall.name}`, toolCall.parameters)
          const result = await toolsExecutor(toolCall.name, toolCall.parameters)
          executedToolCalls.push({
            name: toolCall.name,
            parameters: toolCall.parameters,
            result
          })
          log(`[AIAssistant] Tool executed successfully: ${toolCall.name}`)
        } catch (toolError) {
          log(`[AIAssistant] Tool execution failed: ${toolCall.name}`, toolError)
          executedToolCalls.push({
            name: toolCall.name,
            parameters: toolCall.parameters,
            result: { error: toolError instanceof Error ? toolError.message : 'Unknown error' }
          })
        }
      }
      
      // å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œéœ€è¦ç¬¬äºŒæ¬¡APIè°ƒç”¨è·å–æœ€ç»ˆå›å¤
      if (executedToolCalls.length > 0) {
        const toolResults = executedToolCalls.map(call => 
          `å·¥å…· ${call.name} æ‰§è¡Œç»“æœï¼š\n${JSON.stringify(call.result, null, 2)}`
        ).join('\n\n')
        
        // æ„å»ºç¬¬äºŒæ¬¡è¯·æ±‚
        const secondBody = { ...body }
        switch (config.model) {
          case 'openai':
            secondBody.messages = [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: message },
              { 
                role: 'assistant', 
                content: assistantMessage.content || null,
                tool_calls: assistantMessage.tool_calls || []
              },
              ...executedToolCalls.map(call => ({
                role: 'tool',
                tool_call_id: toolCalls.find(tc => tc.name === call.name)?.id,
                content: JSON.stringify(call.result)
              }))
            ]
            delete secondBody.tools
            delete secondBody.tool_choice
            break
            
          case 'claude':
            secondBody.messages = [
              { role: 'user', content: message },
              { 
                role: 'assistant', 
                content: [
                  ...(Array.isArray(data.content) ? data.content : [data.content]),
                  ...executedToolCalls.map(call => ({
                    type: 'tool_result',
                    tool_use_id: toolCalls.find(tc => tc.name === call.name)?.id,
                    content: JSON.stringify(call.result)
                  }))
                ]
              },
              { role: 'user', content: 'è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœå›ç­”æˆ‘çš„é—®é¢˜ã€‚' }
            ]
            delete secondBody.tools
            break
            
          case 'qwen':
            secondBody.input.messages = [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: message },
              { role: 'assistant', content: assistantMessage.content, tool_calls: assistantMessage.tool_calls },
              ...executedToolCalls.map(call => ({
                role: 'tool',
                name: call.name,
                content: JSON.stringify(call.result)
              }))
            ]
            delete secondBody.input.tools
            break
        }
        
        log('[AIAssistant] Sending follow-up request for final response')
        
        const secondResponse = await fetch(apiUrl, {
          method: 'POST',
          headers,
          body: JSON.stringify(secondBody)
        })
        
        if (secondResponse.ok) {
          const secondData = await secondResponse.json()
          let finalResponse = ''
          
          switch (config.model) {
            case 'openai':
              finalResponse = secondData.choices[0].message.content
              break
            case 'claude':
              if (Array.isArray(secondData.content)) {
                const textContent = secondData.content.find((c: any) => c.type === 'text')
                finalResponse = textContent ? textContent.text : secondData.content[0]?.text || 'æ— æ³•ç”Ÿæˆå›å¤'
              } else {
                finalResponse = secondData.content?.text || 'æ— æ³•ç”Ÿæˆå›å¤'
              }
              break
            case 'qwen':
              finalResponse = secondData.output.choices[0].message.content
              break
          }
          
          log('[AIAssistant] Function calling completed successfully')
          
          return {
            response: finalResponse,
            toolCalls: executedToolCalls
          }
        } else {
          log('[AIAssistant] Follow-up request failed, using tool results directly')
        }
      }
    }
    
    // æ²¡æœ‰å·¥å…·è°ƒç”¨æˆ–å·¥å…·è°ƒç”¨å¤±è´¥æ—¶çš„å›å¤
    let responseText = ''
    switch (config.model) {
      case 'openai':
        responseText = assistantMessage.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚'
        break
      case 'claude':
        responseText = assistantMessage.text || assistantMessage.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚'
        break
      case 'qwen':
        responseText = assistantMessage.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚'
        break
    }
    
    return {
      response: responseText,
      toolCalls: executedToolCalls
    }
    
  } catch (err) {
    error('[AIAssistant] Function calling API error:', err)
    const errorMessage = err instanceof Error ? err.message : 'æœªçŸ¥é”™è¯¯'
    throw new Error(`AIè¯·æ±‚å¤±è´¥: ${errorMessage}`)
  }
} 